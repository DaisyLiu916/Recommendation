{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework1 [written in python 3.7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is the distribution of ratings in the dataset (for ‘review/taste’)? That is, how many 1-star, 2-star, 3-star (etc.) reviews are there? You may write out the values or include a simple plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  #matrix algebra\n",
    "import urllib  #read data from web\n",
    "import scipy.optimize      \n",
    "import random\n",
    "from sklearn import svm  # library for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(fname):\n",
    "  for l in urllib.request.urlopen(fname):\n",
    "    yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print (\"Reading data...\")\n",
    "data = list(parseData(\"http://jmcauley.ucsd.edu/cse190/data/beer/beer_50000.json\"))\n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1.5: 343,\n",
       " 3.0: 4137,\n",
       " 4.5: 12883,\n",
       " 3.5: 8797,\n",
       " 4.0: 16575,\n",
       " 2.0: 1099,\n",
       " 5.0: 4331,\n",
       " 2.5: 1624,\n",
       " 1.0: 211}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "taste_stars={}     \n",
    "# define a dictionary whose key is a rating and value is the number of certain rating\n",
    "for d in data:\n",
    "  if d['review/taste'] not in taste_stars:\n",
    "    taste_stars[d['review/taste']] = 1\n",
    "  else:\n",
    "    taste_stars[d['review/taste']] +=1\n",
    "taste_stars\n",
    "#plt.hist(taste_stars.values(),len(taste_stars));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train a simple predictor to predict a beer’s ‘taste’ score using two features:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$review/taste = θ_0 + θ_1 × [beer\\  is\\  a\\  Hefeweizen] + θ_2 × beer/ABV$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the values of θ0, θ1, and θ2. Briefly describe your interpretation of these values, i.e., what do θ0, θ1, and θ2 represent ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "  feat = [1]\n",
    "  if datum['beer/style'] == 'Hefeweizen':\n",
    "    feat.append(1)\n",
    "  else:\n",
    "    feat.append(0)\n",
    "  feat.append(datum['beer/ABV'])\n",
    "  return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.11795084, -0.05637406,  0.10877902])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X= [feature(d) for d in data]\n",
    "y= [d['review/taste'] for d in data]\n",
    "theta,residuals,rank,s = numpy.linalg.lstsq(X, y,rcond=-1)\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we get $θ_0 = 3.11795084$,$θ_1 = -0.05637406$ and $θ_2 = 0.10877902$. $θ_0$ shows when a beer is not a Hefeiwenzen and beer/ABV=0, the predicted review of taste is 3.12. When beer is a Hefeiweizen, the predicted review of taste is $θ_1$, that is, 0.056 lower in taste reviews. And since $θ_2$ is 0.108, the review of taste increases with the ascending ABV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split the data into two equal fractions – the first half for training, the second half for testing (based on the order they appear in the file). Train the same model as above on the training set only. What is the model’s MSE on the training and on the test set ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.99691466, -0.03573098,  0.11672256])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X=[feature(d) for d in data[:len(data)//2]]  # the first half for training\n",
    "train_y=[d['review/taste'] for d in data[:len(data)//2]]\n",
    "theta,residuals,rank,s = numpy.linalg.lstsq(train_X, train_y,rcond=-1)   \n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(theta, X, y):\n",
    "  theta = numpy.matrix(theta).T\n",
    "  X = numpy.matrix(X)\n",
    "  y = numpy.matrix(y).T\n",
    "  diff = X*theta - y\n",
    "  diffSq = diff.T*diff\n",
    "  mse = diffSq / len(X)    # MSE\n",
    "#  print (\"MSE =\", mse.flatten().tolist())\n",
    "  return mse.flatten().tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.48396805601342413]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(theta, train_X, train_y)    # MSE for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4237065211985415]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the second half for testing the quality of parameters \n",
    "test_X=[feature(d) for d in data[len(data)//2:]]    \n",
    "test_y=[d['review/taste'] for d in data[len(data)//2:]]\n",
    "f(theta, test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model' MSE on the traing set is 0.483968 and on the test set is 0.4237065"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Using the first half for training and the second half for testing may lead to unexpected results (e.g. the training error could be higher than the test error). Repeat the above experiment by using a random 50% split of the data (i.e., half for training, half for testing, after first shuffling the data). Report the MSE on the train and test set, and suggest one possible reason why the result may be different from the previous experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4463668472935334]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(data)   # shuffle the data\n",
    "train_X=[feature(d) for d in data[:len(data)//2]]    # the first half for training\n",
    "train_y=[d['review/taste'] for d in data[:len(data)//2]]\n",
    "theta,residuals,rank,s = numpy.linalg.lstsq(train_X, train_y,rcond=-1)   \n",
    "f(theta, train_X, train_y)    # MSE for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.45299269855573393]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X=[feature(d) for d in data[len(data)//2:]]   \n",
    "test_y=[d['review/taste'] for d in data[len(data)//2:]]\n",
    "f(theta, test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case, the training error is lower than the test error. The unexpected results may come out if the model is under-fitting or if the first half of data have higher noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Modify your experiment from Question 4 to use the features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$review/taste = θ_0 + θ_1 × [ABV\\ if\\ beer\\ is\\ a\\ Hefeweizen] + θ_2 × [ABV\\ if\\ beer\\ is\\ not\\ a\\ Hefeweizen]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### e.g. the first beer in the dataset would have feature [1, 5.0, 0] since the beer is a Hefeweizen. Report the training and testing MSE of this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "  feat = [1]\n",
    "  if datum['beer/style'] == 'Hefeweizen':\n",
    "    feat.append(datum['beer/ABV'])\n",
    "    feat.append(0)\n",
    "  else:\n",
    "    feat.append(0)\n",
    "    feat.append(datum['beer/ABV'])\n",
    "  return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta= [3.1303229  0.09348837 0.10755352]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4463703048302323]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X1=[feature(d) for d in data[:len(data)//2]]\n",
    "train_y1=[d['review/taste'] for d in data[:len(data)//2]]\n",
    "theta,residuals,rank,s = numpy.linalg.lstsq(train_X1, train_y1,rcond=-1)  \n",
    "print('theta=',theta)\n",
    "f(theta, train_X1, train_y1)    # MSE for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.45297443751569955]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X1=[feature(d) for d in data[len(data)//2:]]   \n",
    "test_y1=[d['review/taste'] for d in data[len(data)//2:]]\n",
    "f(theta, test_X1, test_y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. The model from Question 5 uses the same two features as the model from Questions 2-4 and has the same dimensionality. Comment on why the two models might perform differently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model in Q5 shows the influence of Hefeweizen's ABV to the review and other beers' ABV to the review. The model in Q2-4 shows the ABV's influence to review and the influence to review if the beer is Hefeweizen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. First, let’s train a predictor that estimates whether a beer is a ‘Hefeweizen’ using five features describing its rating:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$[‘review/taste’, ‘review/appearance’, ‘review/aroma’, ‘review/palate’, ‘review/overall’].$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train your predictor using an SVM classifier (see the code provided in class). Use a random split of the data as we did in Question 4. Use a regularization constant of C = 1000 as in the code stub. What is the accuracy (percentage of correct classifications) of the predictor on the train and test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "    feat = [datum['review/taste']]\n",
    "    feat.append(datum['review/appearance'])\n",
    "    feat.append(datum['review/aroma'])\n",
    "    feat.append(datum['review/palate'])\n",
    "    feat.append(datum['review/overall'])\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=[feature(d) for d in data[:len(data)//2]]\n",
    "y_train=[d['beer/style']=='Hefeweizen' for d in data[:len(data)//2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a support vector classifier object, with regularization parameter C = 1000\n",
    "clf = svm.SVC(C=1000, kernel='linear')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=[feature(d) for d in data[len(data)//2:]]\n",
    "y_test=[d['beer/style']=='Hefeweizen' for d in data[len(data)//2:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = clf.predict(X_train)\n",
    "test_predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98752"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_train= train_predictions==y_train\n",
    "trainAcc=sum(correct_train)/len(correct_train)\n",
    "trainAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98776"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_test= test_predictions==y_test\n",
    "testAcc=sum(correct_test)/len(correct_test)\n",
    "testAcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Considering same prediction problem as above, can you come up with a more accurate predictor (e.g. using features from the text, or otherwise)? Write down the feature vector you design, and report its train/test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "    feat = [datum['review/taste']]\n",
    "    feat.append(datum['review/appearance'])\n",
    "    feat.append(datum['review/aroma'])\n",
    "    feat.append(datum['review/palate'])\n",
    "    feat.append(datum['review/overall'])\n",
    "    if 'Hefeweizen' in datum['review/text']:\n",
    "        feat.append(1)\n",
    "    else:\n",
    "        feat.append(0)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1=[feature(d) for d in data[:len(data)//2]]\n",
    "y_train1=[d['beer/style']=='Hefeweizen' for d in data[:len(data)//2]]\n",
    "# Create a support vector classifier object, with regularization parameter C = 1000\n",
    "clf = svm.SVC(C=1000, kernel='linear')\n",
    "clf.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test1=[feature(d) for d in data[len(data)//2:]]\n",
    "y_test1=[d['beer/style']=='Hefeweizen' for d in data[len(data)//2:]]\n",
    "train1_predictions = clf.predict(X_train1)\n",
    "test1_predictions = clf.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98852"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_train1= train1_predictions==y_train1\n",
    "trainAcc1=sum(correct_train1)/len(correct_train1)\n",
    "trainAcc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98852"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_test1= test1_predictions==y_test1\n",
    "testAcc1=sum(correct_test1)/len(correct_test1)\n",
    "testAcc1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
