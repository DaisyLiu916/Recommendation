{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "from math import log10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def parseData(fname):\n",
    "  for l in urllib.request.urlopen(fname):\n",
    "    yield eval(l)\n",
    "    \n",
    "### Just the first 5000 reviews\n",
    "\n",
    "print (\"Reading data...\")\n",
    "data = list(parseData(\"http://jmcauley.ucsd.edu/cse190/data/beer/beer_50000.json\"))[:5000]\n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ignore capitalization and remove punctuation\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "for d in data:\n",
    "  r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "  w = r.split()\n",
    "    #w = stemmer.stem(w) # with stemming\n",
    "  for i in range(len(w)-1):\n",
    "    wordCount[w[i]+' '+w[i+1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4587, 'with a'),\n",
       " (2595, 'in the'),\n",
       " (2245, 'of the'),\n",
       " (2056, 'is a'),\n",
       " (2033, 'on the')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_bigram = [(wordCount[i],i) for i in wordCount]\n",
    "frequent_bigram.sort()\n",
    "frequent_bigram.reverse()\n",
    "frequent_bigram[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [x[1] for x in frequent_bigram[:1000]]\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "  feat = [0]*len(words)\n",
    "  r = ''.join([c for c in datum['review/text'].lower() if not c in punctuation])\n",
    "  w = r.split()\n",
    "  for i in range(len(w)-1):\n",
    "    tmp = w[i]+' '+w[i+1]\n",
    "    if tmp in words:\n",
    "      feat[wordId[tmp]] += 1\n",
    "  feat.append(1) #offset\n",
    "  return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [feature(d) for d in data]\n",
    "y = [d['review/overall'] for d in data]\n",
    "\n",
    "#No regularization\n",
    "#theta,residuals,rank,s = numpy.linalg.lstsq(X, y)\n",
    "\n",
    "#With regularization\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False)\n",
    "clf.fit(X, y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3431530140613628"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## mse\n",
    "err = 0\n",
    "for i in range(len(y)):\n",
    "    err += (y[i]-predictions[i])**2\n",
    "err =err/5000\n",
    "err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(term):\n",
    "    num = 0\n",
    "    for d in data:\n",
    "        r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "        if term in r.split():\n",
    "            num += 1\n",
    "    return -log10(num/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(t,d):\n",
    "    num = 0\n",
    "    r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "    for term in r.split():\n",
    "        if term == t:\n",
    "            num += 1\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foam :idf = 1.1378686206869628   tf-idf = 2.2757372413739256\n",
      "smell :idf = 0.5379016188648442   tf-idf = 0.5379016188648442\n",
      "banana :idf = 1.6777807052660807   tf-idf = 3.3555614105321614\n",
      "lactic :idf = 2.9208187539523753   tf-idf = 5.841637507904751\n",
      "tart :idf = 1.8068754016455384   tf-idf = 1.8068754016455384\n"
     ]
    }
   ],
   "source": [
    "terms = ['foam', 'smell', 'banana', 'lactic', 'tart']\n",
    "for term in terms:\n",
    "    print(term + ' :idf = '+str(idf(term))+'   tf-idf = '+str(tf(term,data[0])*idf(term)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in data:\n",
    "  r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "  for w in r.split():\n",
    "    uniCount[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "### donnot run it again\n",
    "word_idf = defaultdict(int)\n",
    "for w in uniCount:\n",
    "    word_idf[w] = idf(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "counts = [(uniCount[w], w) for w in uniCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "words = [x[1] for x in counts]\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)\n",
    "\n",
    "doc1 = data[0]\n",
    "doc2 = data[1]\n",
    "\n",
    "d1 = [0]*len(uniCount)\n",
    "d2 = [0]*len(uniCount)\n",
    "\n",
    "\n",
    "r = ''.join([c for c in doc1['review/text'].lower() if not c in punctuation])\n",
    "for term in r.split():\n",
    "    d1[wordId[term]] = word_idf[term]*tf(term,doc1)\n",
    "    \n",
    "r = ''.join([c for c in doc2['review/text'].lower() if not c in punctuation])\n",
    "for term in r.split():\n",
    "    d2[wordId[term]] = word_idf[term]*tf(term,doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(d1,d2):  \n",
    "    nominator = np.dot(d1,d2)\n",
    "    denominator = sqrt(sum(i**2 for i in d1))*sqrt(sum(i**2 for i in d2))\n",
    "    if denominator == 0 : return 0\n",
    "    cosine = nominator/denominator\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0658819397474438"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(d1,d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72146\n",
      "spicelab\n"
     ]
    }
   ],
   "source": [
    "max_cos = 0\n",
    "beer_id = ''\n",
    "profile_name = ''\n",
    "for d in data[1:]:\n",
    "    d_cur = [0]*len(uniCount)\n",
    "    r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "    for term in r.split():\n",
    "       d_cur[wordId[term]] = word_idf[term]*tf(term,d)\n",
    "    tmp = cosine_similarity(d1,d_cur)\n",
    "    if tmp > max_cos :\n",
    "        max_cos = tmp\n",
    "        beer_id = d['beer/beerId']\n",
    "        profile_name = d['user/profileName']\n",
    "print(beer_id)\n",
    "print(profile_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Just take the most popular words...\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in data:\n",
    "  r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "  for w in r.split():\n",
    "    wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "words = [x[1] for x in counts[:1000]]\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "  feat = [0]*len(words)\n",
    "  r = ''.join([c for c in datum['review/text'].lower() if not c in punctuation])\n",
    "  w = r.split()\n",
    "  for i in w:\n",
    "    tmp = word_idf[i]*tf(i,datum)\n",
    "    if i in wordSet:\n",
    "       feat[wordId[i]] = tmp\n",
    "  feat.append(1) #offset\n",
    "  return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [feature(d) for d in data]\n",
    "y = [d['review/overall'] for d in data]\n",
    "\n",
    "#No regularization\n",
    "#theta,residuals,rank,s = numpy.linalg.lstsq(X, y)\n",
    "\n",
    "#With regularization\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False)\n",
    "clf.fit(X, y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse with tf-idf is: 0.2787595600777218\n"
     ]
    }
   ],
   "source": [
    "## mse\n",
    "err = 0\n",
    "for i in range(len(y)):\n",
    "    err += (y[i]-predictions[i])**2\n",
    "err =err/5000\n",
    "print(\"mse with tf-idf is: \"+str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print (\"Reading data...\")\n",
    "data_all = list(parseData(\"http://jmcauley.ucsd.edu/cse190/data/beer/beer_50000.json\"))\n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_all[:5000]\n",
    "validation = data_all[5000:10000]\n",
    "test = data_all[10000:15000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 1 : unigram, preserve punctuation, word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "### unigram\n",
    "wordCount = defaultdict(int)\n",
    "for d in train:\n",
    "    r = ''.join(c for c in d['review/text'].lower())\n",
    "    for w in r.split():\n",
    "          wordCount[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "words = [x[1] for x in counts[:1000]]\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "  feat = [0]*len(words)\n",
    "  r = ''.join(c for c in datum['review/text'].lower())\n",
    "  for w in r.split():\n",
    "    if w in words:\n",
    "      feat[wordId[w]] += 1\n",
    "  feat.append(1) #offset\n",
    "  return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [feature(d) for d in train]\n",
    "y_train = [d['review/overall'] for d in train]\n",
    "\n",
    "\n",
    "X_validation = [feature(d) for d in validation]\n",
    "y_validation = [d['review/overall'] for d in validation]\n",
    "\n",
    "X_test = [feature(d) for d in test]\n",
    "y_test = [d['review/overall'] for d in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No regularization\n",
    "#theta,residuals,rank,s = numpy.linalg.lstsq(X, y)\n",
    "\n",
    "#With regularization\n",
    "\n",
    "def mse(lamda):\n",
    "   clf = linear_model.Ridge(lamda, fit_intercept=False)\n",
    "   clf.fit(X_train, y_train)\n",
    "   theta = clf.coef_\n",
    "   predictions = clf.predict(X_validation)\n",
    "\n",
    "   ## mse\n",
    "   err = 0\n",
    "   for i in range(len(y_validation)):\n",
    "      err += (y_validation[i]-predictions[i])**2\n",
    "   err =err/5000\n",
    "   return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 0.01  mse of validation: 0.39608492544268364\n",
      "lambda = 0.1  mse of validation: 0.3959317131915895\n",
      "lambda = 1  mse of validation: 0.39446116829442684\n",
      "lambda = 10  mse of validation: 0.3842667979968166\n",
      "lambda = 100  mse of validation: 0.40611767673772076\n"
     ]
    }
   ],
   "source": [
    "print(\"lambda = 0.01  mse of validation: \" +str(mse(0.01)))\n",
    "print(\"lambda = 0.1  mse of validation: \" +str(mse(0.1)))\n",
    "print(\"lambda = 1  mse of validation: \" +str(mse(1)))\n",
    "print(\"lambda = 10  mse of validation: \" +str(mse(10)))\n",
    "print(\"lambda = 100  mse of validation: \" +str(mse(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the mse we obtained, we should choose lambda = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse on the test set is: 0.4032378768323837\n"
     ]
    }
   ],
   "source": [
    "## choose lambda = 10\n",
    "clf = linear_model.Ridge(10, fit_intercept=False)\n",
    "clf.fit(X_train, y_train)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "## mse\n",
    "err = 0\n",
    "for i in range(len(y_test)):\n",
    "      err += (y_test[i]-predictions[i])**2\n",
    "err =err/5000\n",
    "print(\"mse on the test set is: \"+str(err)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 2: bigram, preserve punctuation, word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "for d in train:\n",
    "  r = ''.join(c for c in d['review/text'].lower())\n",
    "  w = r.split()\n",
    "    #w = stemmer.stem(w) # with stemming\n",
    "  for i in range(len(w)-1):\n",
    "    wordCount[w[i]+' '+w[i+1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_bigram = [(wordCount[i],i) for i in wordCount]\n",
    "frequent_bigram.sort()\n",
    "frequent_bigram.reverse()\n",
    "\n",
    "words = [x[1] for x in frequent_bigram[:1000]]\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "  feat = [0]*len(words)\n",
    "  r = ''.join(c for c in datum['review/text'].lower())\n",
    "  w = r.split()\n",
    "  for i in range(len(w)-1):\n",
    "    tmp = w[i]+' '+w[i+1]\n",
    "    if tmp in words:\n",
    "      feat[wordId[tmp]] += 1\n",
    "  feat.append(1) #offset\n",
    "  return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [feature(d) for d in train]\n",
    "y_train = [d['review/overall'] for d in train]\n",
    "\n",
    "\n",
    "X_validation = [feature(d) for d in validation]\n",
    "y_validation = [d['review/overall'] for d in validation]\n",
    "\n",
    "X_test = [feature(d) for d in test]\n",
    "y_test = [d['review/overall'] for d in test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No regularization\n",
    "#theta,residuals,rank,s = numpy.linalg.lstsq(X, y)\n",
    "\n",
    "#With regularization\n",
    "\n",
    "def mse(lamda):\n",
    "   clf = linear_model.Ridge(lamda, fit_intercept=False)\n",
    "   clf.fit(X_train, y_train)\n",
    "   theta = clf.coef_\n",
    "   predictions = clf.predict(X_validation)\n",
    "\n",
    "   ## mse\n",
    "   err = 0\n",
    "   for i in range(len(y_validation)):\n",
    "      err += (y_validation[i]-predictions[i])**2\n",
    "   err =err/5000\n",
    "   return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 0.01  mse of validation: 0.4587312780018877\n",
      "lambda = 0.1  mse of validation: 0.4583889598679832\n",
      "lambda = 1  mse of validation: 0.4552387407523705\n",
      "lambda = 10  mse of validation: 0.4359396997199695\n",
      "lambda = 100  mse of validation: 0.4362181043469898\n"
     ]
    }
   ],
   "source": [
    "print(\"lambda = 0.01  mse of validation: \" +str(mse(0.01)))\n",
    "print(\"lambda = 0.1  mse of validation: \" +str(mse(0.1)))\n",
    "print(\"lambda = 1  mse of validation: \" +str(mse(1)))\n",
    "print(\"lambda = 10  mse of validation: \" +str(mse(10)))\n",
    "print(\"lambda = 100  mse of validation: \" +str(mse(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the performance on validation set, we choose lambda = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse on the test set is: 0.44809417453688355\n"
     ]
    }
   ],
   "source": [
    "## choose lambda = 10\n",
    "clf = linear_model.Ridge(10, fit_intercept=False)\n",
    "clf.fit(X_train, y_train)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "## mse\n",
    "err = 0\n",
    "for i in range(len(y_test)):\n",
    "      err += (y_test[i]-predictions[i])**2\n",
    "err =err/5000\n",
    "print(\"mse on the test set is: \"+str(err)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 3 : unigram, remove punctuation, word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ignore capitalization and remove punctuation\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "for d in train:\n",
    "  r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "  for w in r.split():\n",
    "    #w = stemmer.stem(w) # with stemming\n",
    "    wordCount[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "words = [x[1] for x in counts[:1000]]\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "  feat = [0]*len(words)\n",
    "  r = ''.join([c for c in datum['review/text'].lower() if not c in punctuation])\n",
    "  for w in r.split():\n",
    "    if w in words:\n",
    "      feat[wordId[w]] += 1\n",
    "  feat.append(1) #offset\n",
    "  return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [feature(d) for d in train]\n",
    "y_train = [d['review/overall'] for d in train]\n",
    "\n",
    "\n",
    "X_validation = [feature(d) for d in validation]\n",
    "y_validation = [d['review/overall'] for d in validation]\n",
    "\n",
    "X_test = [feature(d) for d in test]\n",
    "y_test = [d['review/overall'] for d in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No regularization\n",
    "#theta,residuals,rank,s = numpy.linalg.lstsq(X, y)\n",
    "\n",
    "#With regularization\n",
    "\n",
    "def mse(lamda):\n",
    "   clf = linear_model.Ridge(lamda, fit_intercept=False)\n",
    "   clf.fit(X_train, y_train)\n",
    "   theta = clf.coef_\n",
    "   predictions = clf.predict(X_validation)\n",
    "\n",
    "   ## mse\n",
    "   err = 0\n",
    "   for i in range(len(y_validation)):\n",
    "      err += (y_validation[i]-predictions[i])**2\n",
    "   err =err/5000\n",
    "   return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 0.01  mse of validation: 0.379454667980617\n",
      "lambda = 0.1  mse of validation: 0.37930442186825364\n",
      "lambda = 1  mse of validation: 0.3778577799756602\n",
      "lambda = 10  mse of validation: 0.3677593169062544\n",
      "lambda = 100  mse of validation: 0.39213104419367617\n"
     ]
    }
   ],
   "source": [
    "print(\"lambda = 0.01  mse of validation: \" +str(mse(0.01)))\n",
    "print(\"lambda = 0.1  mse of validation: \" +str(mse(0.1)))\n",
    "print(\"lambda = 1  mse of validation: \" +str(mse(1)))\n",
    "print(\"lambda = 10  mse of validation: \" +str(mse(10)))\n",
    "print(\"lambda = 100  mse of validation: \" +str(mse(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choose lambda = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse on the test set is: 0.38943060307690436\n"
     ]
    }
   ],
   "source": [
    "## choose lambda = 10\n",
    "clf = linear_model.Ridge(10, fit_intercept=False)\n",
    "clf.fit(X_train, y_train)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "## mse\n",
    "err = 0\n",
    "for i in range(len(y_test)):\n",
    "      err += (y_test[i]-predictions[i])**2\n",
    "err =err/5000\n",
    "print(\"mse on the test set is: \"+str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 4: bigram, remove punctuation, word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ignore capitalization and remove punctuation\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "for d in train:\n",
    "  r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "  w = r.split()\n",
    "    #w = stemmer.stem(w) # with stemming\n",
    "  for i in range(len(w)-1):\n",
    "    wordCount[w[i]+' '+w[i+1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_bigram = [(wordCount[i],i) for i in wordCount]\n",
    "frequent_bigram.sort()\n",
    "frequent_bigram.reverse()\n",
    "\n",
    "words = [x[1] for x in frequent_bigram[:1000]]\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "  feat = [0]*len(words)\n",
    "  r = ''.join([c for c in datum['review/text'].lower() if not c in punctuation])\n",
    "  w = r.split()\n",
    "  for i in range(len(w)-1):\n",
    "    tmp = w[i]+' '+w[i+1]\n",
    "    if tmp in words:\n",
    "      feat[wordId[tmp]] += 1\n",
    "  feat.append(1) #offset\n",
    "  return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [feature(d) for d in train]\n",
    "y_train = [d['review/overall'] for d in train]\n",
    "\n",
    "\n",
    "X_validation = [feature(d) for d in validation]\n",
    "y_validation = [d['review/overall'] for d in validation]\n",
    "\n",
    "X_test = [feature(d) for d in test]\n",
    "y_test = [d['review/overall'] for d in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No regularization\n",
    "#theta,residuals,rank,s = numpy.linalg.lstsq(X, y)\n",
    "\n",
    "#With regularization\n",
    "\n",
    "def mse(lamda):\n",
    "   clf = linear_model.Ridge(lamda, fit_intercept=False)\n",
    "   clf.fit(X_train, y_train)\n",
    "   theta = clf.coef_\n",
    "   predictions = clf.predict(X_validation)\n",
    "\n",
    "   ## mse\n",
    "   err = 0\n",
    "   for i in range(len(y_validation)):\n",
    "      err += (y_validation[i]-predictions[i])**2\n",
    "   err =err/5000\n",
    "   return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 0.01  mse of validation: 0.4426702835166532\n",
      "lambda = 0.1  mse of validation: 0.44243655374939134\n",
      "lambda = 1  mse of validation: 0.44023705689289544\n",
      "lambda = 10  mse of validation: 0.424838839599379\n",
      "lambda = 100  mse of validation: 0.4273780468358466\n"
     ]
    }
   ],
   "source": [
    "print(\"lambda = 0.01  mse of validation: \" +str(mse(0.01)))\n",
    "print(\"lambda = 0.1  mse of validation: \" +str(mse(0.1)))\n",
    "print(\"lambda = 1  mse of validation: \" +str(mse(1)))\n",
    "print(\"lambda = 10  mse of validation: \" +str(mse(10)))\n",
    "print(\"lambda = 100  mse of validation: \" +str(mse(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose lambda = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse on the test set is: 0.4483852646172084\n"
     ]
    }
   ],
   "source": [
    "## choose lambda = 10\n",
    "clf = linear_model.Ridge(10, fit_intercept=False)\n",
    "clf.fit(X_train, y_train)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "## mse\n",
    "err = 0\n",
    "for i in range(len(y_test)):\n",
    "      err += (y_test[i]-predictions[i])**2\n",
    "err =err/5000\n",
    "print(\"mse on the test set is: \"+str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 5: unigram, remove punctuation, tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_idf(term):\n",
    "    num = 0\n",
    "    for d in train:\n",
    "        r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "        if term in r.split():\n",
    "            num += 1\n",
    "    return -log10(num/len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Just take the most popular words...\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in train:\n",
    "  r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "  for w in r.split():\n",
    "    wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "words = [x[1] for x in counts[:1000]]\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_idf = defaultdict(int)\n",
    "for i in words:\n",
    "    uni_idf[i] = unigram_idf(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "  feat = [0]*len(words)\n",
    "  r = ''.join([c for c in datum['review/text'].lower() if not c in punctuation])\n",
    "  w = r.split()\n",
    "  for i in w: \n",
    "    if i in wordSet:\n",
    "       tmp = uni_idf[i]*tf(i,datum)\n",
    "       feat[wordId[i]] = tmp\n",
    "  feat.append(1) #offset\n",
    "  return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [feature(d) for d in train]\n",
    "y_train = [d['review/overall'] for d in train]\n",
    "\n",
    "\n",
    "X_validation = [feature(d) for d in validation]\n",
    "y_validation = [d['review/overall'] for d in validation]\n",
    "\n",
    "X_test = [feature(d) for d in test]\n",
    "y_test = [d['review/overall'] for d in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No regularization\n",
    "#theta,residuals,rank,s = numpy.linalg.lstsq(X, y)\n",
    "\n",
    "#With regularization\n",
    "\n",
    "def mse(lamda):\n",
    "   clf = linear_model.Ridge(lamda, fit_intercept=False)\n",
    "   clf.fit(X_train, y_train)\n",
    "   theta = clf.coef_\n",
    "   predictions = clf.predict(X_validation)\n",
    "\n",
    "   ## mse\n",
    "   err = 0\n",
    "   for i in range(len(y_validation)):\n",
    "      err += (y_validation[i]-predictions[i])**2\n",
    "   err =err/5000\n",
    "   return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 0.01  mse of validation: 0.3794683914844995\n",
      "lambda = 0.1  mse of validation: 0.37943754775378147\n",
      "lambda = 1  mse of validation: 0.37900731516842195\n",
      "lambda = 10  mse of validation: 0.3755721347449421\n",
      "lambda = 100  mse of validation: 0.40353962517928066\n"
     ]
    }
   ],
   "source": [
    "print(\"lambda = 0.01  mse of validation: \" +str(mse(0.01)))\n",
    "print(\"lambda = 0.1  mse of validation: \" +str(mse(0.1)))\n",
    "print(\"lambda = 1  mse of validation: \" +str(mse(1)))\n",
    "print(\"lambda = 10  mse of validation: \" +str(mse(10)))\n",
    "print(\"lambda = 100  mse of validation: \" +str(mse(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose lambda = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse on the test set is: 0.398328745046825\n"
     ]
    }
   ],
   "source": [
    "## choose lambda = 10\n",
    "clf = linear_model.Ridge(10, fit_intercept=False)\n",
    "clf.fit(X_train, y_train)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "## mse\n",
    "err = 0\n",
    "for i in range(len(y_test)):\n",
    "      err += (y_test[i]-predictions[i])**2\n",
    "err =err/5000\n",
    "print(\"mse on the test set is: \"+str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 6: unigram, preserve punctuation, tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_idf2(term):\n",
    "    num = 0\n",
    "    for d in train:\n",
    "        r = ''.join(c for c in d['review/text'].lower())\n",
    "        if term in r.split():\n",
    "            num += 1\n",
    "    return -log10(num/len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_preserve(t,d):\n",
    "    num = 0\n",
    "    r = ''.join(c for c in d['review/text'].lower())\n",
    "    for term in r.split():\n",
    "        if term == t:\n",
    "            num += 1\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Just take the most popular words...\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in train:\n",
    "  r = ''.join(c for c in d['review/text'].lower())\n",
    "  for w in r.split():\n",
    "    wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "words = [x[1] for x in counts[:1000]]\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_idf2 = defaultdict(int)\n",
    "for i in words:\n",
    "    uni_idf2[i] = unigram_idf2(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "  feat = [0]*len(words)\n",
    "  r = ''.join(c for c in datum['review/text'].lower())\n",
    "  w = r.split()\n",
    "  for i in w:\n",
    "    if i in wordSet:\n",
    "       tmp = uni_idf2[i]*tf_preserve(i,datum)\n",
    "       feat[wordId[i]] = tmp\n",
    "  feat.append(1) #offset\n",
    "  return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [feature(d) for d in train]\n",
    "y_train = [d['review/overall'] for d in train]\n",
    "\n",
    "\n",
    "X_validation = [feature(d) for d in validation]\n",
    "y_validation = [d['review/overall'] for d in validation]\n",
    "\n",
    "X_test = [feature(d) for d in test]\n",
    "y_test = [d['review/overall'] for d in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 0.01  mse of validation: 0.3960970367051074\n",
      "lambda = 0.1  mse of validation: 0.3960494003577027\n",
      "lambda = 1  mse of validation: 0.3954696222066391\n",
      "lambda = 10  mse of validation: 0.39089098243413944\n",
      "lambda = 100  mse of validation: 0.41681817393572\n"
     ]
    }
   ],
   "source": [
    "#No regularization\n",
    "#theta,residuals,rank,s = numpy.linalg.lstsq(X, y)\n",
    "\n",
    "#With regularization\n",
    "\n",
    "def mse(lamda):\n",
    "   clf = linear_model.Ridge(lamda, fit_intercept=False)\n",
    "   clf.fit(X_train, y_train)\n",
    "   theta = clf.coef_\n",
    "   predictions = clf.predict(X_validation)\n",
    "\n",
    "   ## mse\n",
    "   err = 0\n",
    "   for i in range(len(y_validation)):\n",
    "      err += (y_validation[i]-predictions[i])**2\n",
    "   err =err/5000\n",
    "   return err\n",
    "\n",
    "print(\"lambda = 0.01  mse of validation: \" +str(mse(0.01)))\n",
    "print(\"lambda = 0.1  mse of validation: \" +str(mse(0.1)))\n",
    "print(\"lambda = 1  mse of validation: \" +str(mse(1)))\n",
    "print(\"lambda = 10  mse of validation: \" +str(mse(10)))\n",
    "print(\"lambda = 100  mse of validation: \" +str(mse(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose lambda = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse on the test set is: 0.4106750195508106\n"
     ]
    }
   ],
   "source": [
    "## choose lambda = 10\n",
    "clf = linear_model.Ridge(10, fit_intercept=False)\n",
    "clf.fit(X_train, y_train)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "## mse\n",
    "err = 0\n",
    "for i in range(len(y_test)):\n",
    "      err += (y_test[i]-predictions[i])**2\n",
    "err =err/5000\n",
    "print(\"mse on the test set is: \"+str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 7: bigram, remove punctuation, tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_idf(term):\n",
    "    num = 0\n",
    "    for d in train:\n",
    "        r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "        w = r.split()\n",
    "        for i in range(len(w)-1):\n",
    "            if (w[i]+' '+w[i+1]) == term:\n",
    "                num += 1\n",
    "    return -log10(num/len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_tf(t,d):\n",
    "    num = 0\n",
    "    r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "    w= r.split()\n",
    "    for i in range(len(w)-1):\n",
    "        if (w[i]+' '+w[i+1]) == t:\n",
    "            num += 1\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ignore capitalization and remove punctuation\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "for d in train:\n",
    "  r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "  w = r.split()\n",
    "    #w = stemmer.stem(w) # with stemming\n",
    "  for i in range(len(w)-1):\n",
    "    wordCount[w[i]+' '+w[i+1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_bigram = [(wordCount[i],i) for i in wordCount]\n",
    "frequent_bigram.sort()\n",
    "frequent_bigram.reverse()\n",
    "\n",
    "words = [x[1] for x in frequent_bigram[:1000]]\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_idf = defaultdict(int)\n",
    "for tmp in words:\n",
    "    bi_idf[tmp] = bigram_idf(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "  feat = [0]*len(words)\n",
    "  r = ''.join([c for c in datum['review/text'].lower() if not c in punctuation])\n",
    "  w = r.split()\n",
    "  for i in range(len(w)-1):\n",
    "    tmp = w[i]+' '+w[i+1]\n",
    "    if tmp in words:\n",
    "      feat[wordId[tmp]] = bi_idf[tmp]*bigram_tf(tmp,datum)\n",
    "  feat.append(1) #offset\n",
    "  return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [feature(d) for d in train]\n",
    "y_train = [d['review/overall'] for d in train]\n",
    "\n",
    "\n",
    "X_validation = [feature(d) for d in validation]\n",
    "y_validation = [d['review/overall'] for d in validation]\n",
    "\n",
    "X_test = [feature(d) for d in test]\n",
    "y_test = [d['review/overall'] for d in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 0.01  mse of validation: 0.44268492861956155\n",
      "lambda = 0.1  mse of validation: 0.44258261376942376\n",
      "lambda = 1  mse of validation: 0.44166302414275777\n",
      "lambda = 10  mse of validation: 0.4349377766043576\n",
      "lambda = 100  mse of validation: 0.441876929275772\n"
     ]
    }
   ],
   "source": [
    "#No regularization\n",
    "#theta,residuals,rank,s = numpy.linalg.lstsq(X, y)\n",
    "\n",
    "#With regularization\n",
    "\n",
    "def mse(lamda):\n",
    "   clf = linear_model.Ridge(lamda, fit_intercept=False)\n",
    "   clf.fit(X_train, y_train)\n",
    "   theta = clf.coef_\n",
    "   predictions = clf.predict(X_validation)\n",
    "\n",
    "   ## mse\n",
    "   err = 0\n",
    "   for i in range(len(y_validation)):\n",
    "      err += (y_validation[i]-predictions[i])**2\n",
    "   err =err/5000\n",
    "   return err\n",
    "\n",
    "print(\"lambda = 0.01  mse of validation: \" +str(mse(0.01)))\n",
    "print(\"lambda = 0.1  mse of validation: \" +str(mse(0.1)))\n",
    "print(\"lambda = 1  mse of validation: \" +str(mse(1)))\n",
    "print(\"lambda = 10  mse of validation: \" +str(mse(10)))\n",
    "print(\"lambda = 100  mse of validation: \" +str(mse(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choose lambda = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse on the test set is: 0.46019509380338913\n"
     ]
    }
   ],
   "source": [
    "## choose lambda = 10\n",
    "clf = linear_model.Ridge(10, fit_intercept=False)\n",
    "clf.fit(X_train, y_train)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "## mse\n",
    "err = 0\n",
    "for i in range(len(y_test)):\n",
    "      err += (y_test[i]-predictions[i])**2\n",
    "err =err/5000\n",
    "print(\"mse on the test set is: \"+str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 8 : bigram, preserve punctuation, tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_idf2(term):\n",
    "    num = 0\n",
    "    for d in train:\n",
    "        r = ''.join(c for c in d['review/text'].lower())\n",
    "        w = r.split()\n",
    "        for i in range(len(w)-1):\n",
    "            if (w[i]+' '+w[i+1]) == term:\n",
    "                num += 1\n",
    "    return -log10(num/len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_tf2(t,d):\n",
    "    num = 0\n",
    "    r = ''.join(c for c in d['review/text'].lower())\n",
    "    w= r.split()\n",
    "    for i in range(len(w)-1):\n",
    "        if (w[i]+' '+w[i+1]) == t:\n",
    "            num += 1\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "for d in train:\n",
    "  r = ''.join(c for c in d['review/text'].lower())\n",
    "  w = r.split()\n",
    "    #w = stemmer.stem(w) # with stemming\n",
    "  for i in range(len(w)-1):\n",
    "    wordCount[w[i]+' '+w[i+1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_bigram = [(wordCount[i],i) for i in wordCount]\n",
    "frequent_bigram.sort()\n",
    "frequent_bigram.reverse()\n",
    "\n",
    "words = [x[1] for x in frequent_bigram[:1000]]\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_idf2 = defaultdict(int)\n",
    "for tmp in words:\n",
    "    bi_idf2[tmp] = bigram_idf2(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "  feat = [0]*len(words)\n",
    "  r = ''.join(c for c in datum['review/text'].lower())\n",
    "  w = r.split()\n",
    "  for i in range(len(w)-1):\n",
    "    tmp = w[i]+' '+w[i+1]\n",
    "    if tmp in words:\n",
    "      feat[wordId[tmp]] = bi_idf2[tmp]*bigram_tf2(tmp,datum)\n",
    "  feat.append(1) #offset\n",
    "  return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [feature(d) for d in train]\n",
    "y_train = [d['review/overall'] for d in train]\n",
    "\n",
    "\n",
    "X_validation = [feature(d) for d in validation]\n",
    "y_validation = [d['review/overall'] for d in validation]\n",
    "\n",
    "X_test = [feature(d) for d in test]\n",
    "y_test = [d['review/overall'] for d in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 0.01  mse of validation: 0.4587524316647613\n",
      "lambda = 0.1  mse of validation: 0.45860018942688857\n",
      "lambda = 1  mse of validation: 0.45725953885052295\n",
      "lambda = 10  mse of validation: 0.4482363334483097\n",
      "lambda = 100  mse of validation: 0.4495139426053207\n"
     ]
    }
   ],
   "source": [
    "#No regularization\n",
    "#theta,residuals,rank,s = numpy.linalg.lstsq(X, y)\n",
    "\n",
    "#With regularization\n",
    "\n",
    "def mse(lamda):\n",
    "   clf = linear_model.Ridge(lamda, fit_intercept=False)\n",
    "   clf.fit(X_train, y_train)\n",
    "   theta = clf.coef_\n",
    "   predictions = clf.predict(X_validation)\n",
    "\n",
    "   ## mse\n",
    "   err = 0\n",
    "   for i in range(len(y_validation)):\n",
    "      err += (y_validation[i]-predictions[i])**2\n",
    "   err =err/5000\n",
    "   return err\n",
    "\n",
    "print(\"lambda = 0.01  mse of validation: \" +str(mse(0.01)))\n",
    "print(\"lambda = 0.1  mse of validation: \" +str(mse(0.1)))\n",
    "print(\"lambda = 1  mse of validation: \" +str(mse(1)))\n",
    "print(\"lambda = 10  mse of validation: \" +str(mse(10)))\n",
    "print(\"lambda = 100  mse of validation: \" +str(mse(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose lambda = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse on the test set is: 0.4600827802914737\n"
     ]
    }
   ],
   "source": [
    "## choose lambda = 10\n",
    "clf = linear_model.Ridge(10, fit_intercept=False)\n",
    "clf.fit(X_train, y_train)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "## mse\n",
    "err = 0\n",
    "for i in range(len(y_test)):\n",
    "      err += (y_test[i]-predictions[i])**2\n",
    "err =err/5000\n",
    "print(\"mse on the test set is: \"+str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summary of 8 models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Unigram/bigram |Remove/preserve punctuation |tf-idf/word counts| mse on test set|\n",
    "|--------------|-----------------------|--------------------|--------------------|\n",
    "|Unigram |has punctuation |word counts|0.4032|\n",
    "|bigram |has |word counts|0.4481|\n",
    "|Unigram|no|word counts|0.3894|\n",
    "|bigram|no|word counts|0.4484|\n",
    "|Unigram|no|tf-idf|0.3983|\n",
    "|Unigram|has|tf-idf|0.4107|\n",
    "|bigram|no|tf-idf|0.4602|\n",
    "|bigram|has|tf-idf|0.4601|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the comparison, model with unigram, tf-idf and no punctuation has the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
